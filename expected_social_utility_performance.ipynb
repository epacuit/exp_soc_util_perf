{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32ce4e85",
   "metadata": {},
   "source": [
    "# Expected Social Utility Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a9261b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pref_voting.voting_methods import *\n",
    "from pref_voting.probabilistic_methods import *\n",
    "from pref_voting.generate_spatial_profiles import *\n",
    "from pref_voting.generate_profiles import *\n",
    "from pref_voting.generate_utility_profiles import *\n",
    "from pref_voting.utility_methods import *\n",
    "from pref_voting.utility_functions import *\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocess import Pool, cpu_count, current_process\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial \n",
    "import datetime\n",
    "import time\n",
    "import pref_voting\n",
    "from pref_voting.grade_methods import *\n",
    "from numba import jit\n",
    "# needed to ensure that random numbers are different in each process (for multiprocessing)\n",
    "import os\n",
    "import time\n",
    "#from memory_profiler import profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpus = cpu_count() - 2\n",
    "print(f\"Number of CPUs: {num_cpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da3c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pref_voting\n",
    "pref_voting.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41167e45",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5935ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_utility(prob, util_func):\n",
    "    return sum([prob[c] * util_func(c) for c in prob.keys()])\n",
    "\n",
    "def to_linear_prof(uprof): \n",
    "    return Profile([sorted(uprof.domain, key=lambda x: u(x), reverse=True) for u in uprof.utilities])\n",
    "\n",
    "def find_winning_probs(vms, prob_vms, prof): \n",
    "    prob_ws =  {vm.name: vm.prob(prof) for vm in vms}\n",
    "    for vm in prob_vms:\n",
    "        prob_ws[vm.name] = vm(prof)\n",
    "    return prob_ws\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def soc_util_performance(\n",
    "        avg_util_of_util_ws, \n",
    "        avg_util_of_cand, \n",
    "        avg_util_of_vm_ws):\n",
    "\n",
    "    return (avg_util_of_vm_ws - avg_util_of_cand) / (avg_util_of_util_ws - avg_util_of_cand)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96b55408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimated_variance_of_sampling_dist(mean_for_each_vm, values_for_each_vm):\n",
    "    # values_for_each_vm is a 2d numpy array\n",
    "\n",
    "    m = values_for_each_vm.shape[1]\n",
    "\n",
    "    row_means_reshaped = mean_for_each_vm[:, np.newaxis]\n",
    "    return  (1/(m*(m-1))) * np.sum((values_for_each_vm - row_means_reshaped)**2, axis=1)\n",
    "\n",
    "def estimated_std_error(mean_for_each_vm, values_for_each_vm):\n",
    "    # values_for_each_vm is a 2d numpy array\n",
    "    return np.sqrt(estimated_variance_of_sampling_dist(mean_for_each_vm, values_for_each_vm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8e359",
   "metadata": {},
   "source": [
    "## Main Simulation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57c6e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples_randomly_polarized_voters(\n",
    "        num_polarized_cands, \n",
    "        num_centrist_cands,\n",
    "        num_voters,\n",
    "        prob_centrist_voter, \n",
    "        vms, \n",
    "        prob_vms, \n",
    "        grade_vms,\n",
    "        num_dims, \n",
    "        cand_cov,\n",
    "        voter_cov,\n",
    "        normalization,\n",
    "        voter_utility,\n",
    "        num_dims_polarized,\n",
    "        polarization_distance,\n",
    "        num_profiles): \n",
    "    \n",
    "    cand_cov = cand_cov if cand_cov is not None else np.eye(num_dims)\n",
    "    voter_cov = voter_cov if voter_cov is not None else np.eye(num_dims)\n",
    "\n",
    "    num_left_cands = num_polarized_cands // 2\n",
    "    sprofs = generate_spatial_profile_polarized_cands_randomly_polarized_voters(\n",
    "        [\n",
    "            (np.array([polarization_distance] * num_dims_polarized +  [0] * (num_dims - num_dims_polarized)), cand_cov, num_left_cands),\n",
    "\n",
    "            (np.array([-1 * polarization_distance] * num_dims_polarized + [0] * (num_dims - num_dims_polarized)), cand_cov, num_polarized_cands - num_left_cands),\n",
    "\n",
    "            (np.array([0] * num_dims), cand_cov, num_centrist_cands)],\n",
    "        num_voters,\n",
    "        [\n",
    "            (np.array([polarization_distance] * num_dims_polarized + [0] * (num_dims - num_dims_polarized)), voter_cov, (1 - prob_centrist_voter) / 2),\n",
    "\n",
    "            (np.array([-1 * polarization_distance] * num_dims_polarized + [0] * (num_dims - num_dims_polarized)), voter_cov, (1 - prob_centrist_voter) / 2), \n",
    "            \n",
    "            (np.array([0] * num_dims), voter_cov, prob_centrist_voter)],\n",
    "             \n",
    "        num_profiles = num_profiles)\n",
    "\n",
    "    _uprofs = [sprof.to_utility_profile(utility_function=voter_utility) for sprof in sprofs]\n",
    "\n",
    "    del sprofs\n",
    "\n",
    "    if normalization == 'range':\n",
    "        uprofs = [_uprof.normalize_by_range() for _uprof in _uprofs]\n",
    "    elif normalization == 'score':\n",
    "        uprofs = [_uprof.normalize_by_standard_score() for _uprof in _uprofs]\n",
    "    else: \n",
    "        uprofs = _uprofs\n",
    "\n",
    "    profs = [to_linear_prof(uprof) for uprof in uprofs]\n",
    "\n",
    "    candidates = profs[0].candidates\n",
    "\n",
    "    util_ws_s = [sum_utilitarian(uprof) for uprof in uprofs]\n",
    "    avg_utils = [uprof.avg_utility_function() for uprof in uprofs]\n",
    "\n",
    "    find_winning_probs_partial = partial(find_winning_probs, vms, prob_vms)\n",
    "\n",
    "    winning_prob_dicts = list(map(find_winning_probs_partial, profs)) \n",
    "\n",
    "    if len(grade_vms) > 0:\n",
    "        approval_profs = [uprof.to_approval_profile() for uprof in uprofs]\n",
    "        find_grade_winning_probs_partial = partial(find_winning_probs, grade_vms, [])\n",
    "        grade_winning_prob_dicts = list(map(find_grade_winning_probs_partial, approval_profs))\n",
    "        for i in range(len(winning_prob_dicts)):\n",
    "            winning_prob_dicts[i].update(grade_winning_prob_dicts[i])\n",
    "            \n",
    "        del approval_profs\n",
    "\n",
    "    del profs # free up memory\n",
    "\n",
    "    avg_util_of_util_ws = [np.average([avg_utils[uidx](w) for w in util_ws]) for uidx, util_ws in enumerate(util_ws_s)]\n",
    "    avg_util_of_cand = [np.average([avg_util(c) for c in candidates]) for avg_util in avg_utils]\n",
    "\n",
    "    return np.array([np.array([avg_util_of_util_ws[pidx]] + [avg_util_of_cand[pidx]] + [expected_utility(winning_prob_dicts[pidx][vm.name], avg_utils[pidx]) for vm in vms + prob_vms + grade_vms]) for pidx in range(num_profiles)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a436e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim_with_estimated_standard_error(\n",
    "        generate_samples, \n",
    "        max_std_error, \n",
    "        initial_trials=1000, \n",
    "        step_trials=1000,\n",
    "        min_num_trials = 10000\n",
    "        ):\n",
    "    \n",
    "    results = generate_samples(num_profiles = initial_trials)\n",
    "    avg_util_of_util_ws, avg_util_of_cand, *avg_util_of_vm_ws = results.T\n",
    "\n",
    "    del results  # deallocate memory for results\n",
    "\n",
    "    soc_util_performance_values = np.array([soc_util_performance(avg_util_of_util_ws, avg_util_of_cand, avg_u) for avg_u in avg_util_of_vm_ws])\n",
    "    \n",
    "    mean_s = np.mean(soc_util_performance_values, axis=1)\n",
    "    est_std_errors = estimated_std_error(mean_s, soc_util_performance_values)\n",
    "\n",
    "    num_trials = initial_trials\n",
    "    \n",
    "    while np.any(est_std_errors > max_std_error) or (num_trials < min_num_trials):\n",
    "\n",
    "        new_results = generate_samples(num_profiles = step_trials)\n",
    "\n",
    "        new_avg_util_of_util_ws, new_avg_util_of_cand, *new_avg_util_of_vm_ws = new_results.T\n",
    "\n",
    "        num_trials += step_trials\n",
    "\n",
    "        avg_util_of_util_ws = np.concatenate((avg_util_of_util_ws, new_avg_util_of_util_ws), axis=0)\n",
    "\n",
    "        avg_util_of_cand = np.concatenate((avg_util_of_cand, new_avg_util_of_cand), axis=0)\n",
    "\n",
    "        avg_util_of_vm_ws = np.array([np.concatenate((a, b)) for a, b in zip(avg_util_of_vm_ws, new_avg_util_of_vm_ws)])\n",
    "        \n",
    "        soc_util_performance_values = np.array([soc_util_performance(avg_util_of_util_ws, avg_util_of_cand, avg_su)\n",
    "                            for avg_su in avg_util_of_vm_ws])\n",
    "        \n",
    "        mean_s = np.mean(soc_util_performance_values, axis=1)\n",
    "        variance_s = np.var(soc_util_performance_values, axis=1)\n",
    "        est_std_errors = estimated_std_error(mean_s, soc_util_performance_values)\n",
    "        #print(f\"mean after {num_trials}\"  , mean_s)\n",
    "        #print(f\"est_std_error after {num_trials}\"  , est_std_errors)\n",
    "\n",
    "    return mean_s, est_std_errors, variance_s, num_trials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57969839",
   "metadata": {},
   "source": [
    "## Setting Simulation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69ba4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pref_voting_version = pref_voting.__version__\n",
    "\n",
    "all_num_cands =   [3, 4, 5, 6, 10, 7, 8, 9] \n",
    "all_num_voters = [11, 101, 1001]\n",
    "all_num_dims =    [1, 2, 4, 8]\n",
    "all_normalizations = [\"none\", \"range\"]\n",
    "all_is_polarized = [False, True]\n",
    "all_num_centrist_cands = [\"none\", \"half\", \"all\"]\n",
    "all_prob_centrist_voters = [0.0, 0.5, 1.0]\n",
    "all_num_dims_polarized = [\"one\", \"half\", \"all\"]\n",
    "all_polarization_distances = [1]\n",
    "all_subpopulation_stds = [1, 0.5]\n",
    "all_dispersion =  [1, 0.5]\n",
    "all_correlation = [0, 0.5]\n",
    "all_voter_utilities = {\n",
    "    \"Linear\": linear_utility, \n",
    "    \"Quadratic\": quadratic_utility, \n",
    "    \"Shepsle\": shepsle_utility,\n",
    "    \"Matthews\": matthews_utility,\n",
    "    \"Mixed Proximity-RM\": mixed_rm_utility,\n",
    "    \"RM\": rm_utility    \n",
    "    }\n",
    "\n",
    "vms = [\n",
    "    # plurality_veto,\n",
    "    # random_consensus_builder_st,\n",
    "    # MLRCB,\n",
    "    # MLRaDiUS,\n",
    "    # bracket_voting,\n",
    "    # condorcet_plurality,\n",
    "    # knockout,\n",
    "    # loss_trimmer,\n",
    "    # river_zt,\n",
    "    # smith_set,\n",
    "    # superior_voting\n",
    "    # condorcet,\n",
    "    # copeland,\n",
    "    # copeland_local_borda,\n",
    "    # copeland_global_borda,\n",
    "    # plurality,\n",
    "    # anti_plurality,\n",
    "    # borda,\n",
    "    # instant_runoff,\n",
    "    # plurality_with_runoff_put,\n",
    "    # benham, \n",
    "    # bottom_two_runoff_instant_runoff,\n",
    "    # coombs,\n",
    "    # baldwin,\n",
    "    # weak_nanson,\n",
    "    # raynaud,\n",
    "    # minimax,\n",
    "    # stable_voting,\n",
    "    # beat_path_Floyd_Warshall,\n",
    "    # ranked_pairs_zt,\n",
    "    # split_cycle,\n",
    "    # daunou,\n",
    "    # blacks,\n",
    "    # condorcet_irv,\n",
    "    # smith_irv, \n",
    "    # bucklin,\n",
    "    # woodall, \n",
    "    # river_zt,\n",
    "    # smith_minimax, \n",
    "    # tideman_alternative_smith\n",
    "]\n",
    "prob_vms = [\n",
    "    # random_dictator,\n",
    "    # pr_borda,\n",
    "    # maximal_lottery, \n",
    "    # RaDiUS    \n",
    "]\n",
    "grade_vms = [\n",
    "    # approval\n",
    "]\n",
    "\n",
    "max_std_error = 0.005\n",
    "initial_trials = 1000 \n",
    "step_trials = 1000\n",
    "min_num_trials = 10000\n",
    "\n",
    "num_cands_to_num_centrists = {nc: sorted(list(set([{\"none\": 0, \"one\": 1, \"half\": nc // 2, \"all\": nc}[cent] for cent in all_num_centrist_cands]))) for nc in all_num_cands}\n",
    "\n",
    "dim_to_num_dims_polarized = {\n",
    "    nd: sorted([_n for _n in list(set([{\n",
    "        \"one\": 1, \n",
    "        \"half\": nd // 2, \n",
    "        \"all\": nd}[pol] \n",
    "        for pol in all_num_dims_polarized])) \n",
    "        if _n > 0]) for nd in all_num_dims}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2eb1dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "polarized_keys_for_fn = []\n",
    "unpolarized_keys_for_fn = []\n",
    "\n",
    "polarized_keys = []\n",
    "unpolarized_keys = []\n",
    "\n",
    "for normalization in all_normalizations: \n",
    "    for num_voters in all_num_voters: \n",
    "        for num_cands in all_num_cands: \n",
    "            for num_dims in all_num_dims: \n",
    "                for dispersion in all_dispersion: \n",
    "                    for correlation in all_correlation: \n",
    "                        for voter_utility_name, voter_utility in all_voter_utilities.items():\n",
    "                            if num_dims == 1 and voter_utility_name == \"Matthews\":\n",
    "                                continue\n",
    "                            if dispersion == 1: \n",
    "                                voter_cov = generate_covariance(num_dims, 1, correlation)\n",
    "                                cand_cov = voter_cov\n",
    "                            elif dispersion == 0.5:\n",
    "                                voter_cov = generate_covariance(num_dims, 1, correlation)\n",
    "                                cand_cov = generate_covariance(num_dims, 0.5, correlation)\n",
    "                   \n",
    "                            for is_polarized in all_is_polarized:\n",
    "                                if not is_polarized: \n",
    "                                    num_centrist_cands = 0\n",
    "                                    prob_centrist_voters = 0.0\n",
    "                                    num_dims_polarized = 0\n",
    "                                    polarization_distance = 0\n",
    "                                    unpolarized_keys.append((\n",
    "                                        num_cands,\n",
    "                                        num_voters,\n",
    "                                        num_dims,\n",
    "                                        correlation,\n",
    "                                        dispersion,\n",
    "                                        num_dims_polarized,\n",
    "                                        1, # subpopulation_std\n",
    "                                        \"None\", #polarization_distance,\n",
    "                                        \"None\", #num_centrist_cands,\n",
    "                                        \"None\", #prob_centrist_voters,\n",
    "                                        voter_utility_name,\n",
    "                                        normalization))\n",
    "\n",
    "                                    unpolarized_keys_for_fn.append((\n",
    "                                        num_cands - num_centrist_cands,\n",
    "                                        num_centrist_cands, \n",
    "                                        num_voters,\n",
    "                                        prob_centrist_voters, \n",
    "                                        vms,\n",
    "                                        prob_vms,\n",
    "                                        grade_vms,\n",
    "                                        num_dims,\n",
    "                                        cand_cov,\n",
    "                                        voter_cov, \n",
    "                                        normalization,\n",
    "                                        voter_utility,\n",
    "                                        num_dims_polarized,\n",
    "                                        polarization_distance,))\n",
    "\n",
    "                                else: # is_polarized is True\n",
    "                                    for num_dims_polarized in dim_to_num_dims_polarized[num_dims]:\n",
    "                                        for num_centrist_cands in num_cands_to_num_centrists[num_cands]: \n",
    "                                            for prob_centrist_voters in all_prob_centrist_voters:\n",
    "                                                if num_centrist_cands == num_cands and prob_centrist_voters == 1.0:\n",
    "                                                    continue\n",
    "                                                for polarization_distance in all_polarization_distances:\n",
    "                                                    for subpop_std in all_subpopulation_stds:\n",
    "                                                        if dispersion == 1: \n",
    "                                                            voter_cov = generate_covariance(num_dims, subpop_std, correlation)\n",
    "                                                            cand_cov = voter_cov\n",
    "                                                        elif dispersion == 0.5:\n",
    "                                                            voter_cov = generate_covariance(num_dims, subpop_std, correlation)\n",
    "                                                            cand_cov = generate_covariance(num_dims, 0.5 * subpop_std, correlation)\n",
    "                                                        polarized_keys.append((  \n",
    "                                                            num_cands,\n",
    "                                                            num_voters,\n",
    "                                                            num_dims,\n",
    "                                                            correlation,\n",
    "                                                            dispersion,\n",
    "                                                            num_dims_polarized,\n",
    "                                                            subpop_std,\n",
    "                                                            polarization_distance,\n",
    "                                                            num_centrist_cands,\n",
    "                                                            prob_centrist_voters,\n",
    "                                                            voter_utility_name,\n",
    "                                                            normalization))\n",
    "\n",
    "                                                        polarized_keys_for_fn.append((\n",
    "                                                            num_cands - num_centrist_cands,\n",
    "                                                            num_centrist_cands, \n",
    "                                                            num_voters, \n",
    "                                                            prob_centrist_voters,\n",
    "                                                            vms,\n",
    "                                                            prob_vms,\n",
    "                                                            grade_vms,\n",
    "                                                            num_dims,\n",
    "                                                            cand_cov,\n",
    "                                                            voter_cov, \n",
    "                                                            normalization,\n",
    "                                                            voter_utility,\n",
    "                                                            num_dims_polarized,\n",
    "                                                            polarization_distance,))\n",
    "                                                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbfae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_keys = unpolarized_keys + polarized_keys\n",
    "print(\"Total number of keys: \", len(all_keys))\n",
    "all_keys_for_fn = unpolarized_keys_for_fn + polarized_keys_for_fn\n",
    "\n",
    "# divide all_keys into chunks of size 100\n",
    "all_keys_for_fn_chunks = [all_keys_for_fn[i:i + 100] for i in range(0, len(all_keys_for_fn), 100)]\n",
    "\n",
    "all_keys_chunks = [all_keys[i:i + 100] for i in range(0, len(all_keys), 100)]\n",
    "\n",
    "print(\"Total number of chunks: \", len(all_keys_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4521f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim_with_params(key): \n",
    "    exp = partial(generate_samples_randomly_polarized_voters, *key)\n",
    "    return run_sim_with_estimated_standard_error(\n",
    "        exp, \n",
    "        max_std_error, \n",
    "        initial_trials=initial_trials, \n",
    "        step_trials=step_trials,\n",
    "        min_num_trials=min_num_trials)\n",
    "    # mean_s, half_width_s, variance_bootstrap_means, variance_soc_util_performance_values, num_trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2662b29d",
   "metadata": {},
   "source": [
    "### Main Simulation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1477459",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_chunk_idx = 0 \n",
    "data_dir  = 'data'\n",
    "\n",
    "for idx, (chunk, chunk_for_fn) in enumerate(zip(all_keys_chunks[starting_chunk_idx::], all_keys_for_fn_chunks[starting_chunk_idx::])):\n",
    "    chunk_num = idx + starting_chunk_idx\n",
    "    print(\"starting chunk\", chunk_num)\n",
    "    print(chunk)\n",
    "    with Pool(num_cpus) as pool:\n",
    "        results = pool.map(run_sim_with_params, chunk_for_fn)\n",
    "    print(\"RESULTS \", results)\n",
    "    data_for_df = {\n",
    "                \"num_cands\": [],\n",
    "                \"num_voters\": [],\n",
    "                \"num_dims\": [],\n",
    "                \"correlation\": [],\n",
    "                \"rel_dispersion\": [],\n",
    "                \"num_dims_polarized\": [],\n",
    "                \"subpopulation_std\": [],\n",
    "                \"polarization_distance\": [],\n",
    "                \"num_centrist_cands\": [],\n",
    "                \"prob_centrist_voters\": [],\n",
    "                \"voter_utility\": [],\n",
    "                \"normalization\": [],\n",
    "                \"initial_trials\": [],\n",
    "                \"step_trials\": [],\n",
    "                \"min_num_trials\": [],\n",
    "                \"max_std_error\": [],\n",
    "                \"num_trials\": [],\n",
    "                \"vm\": [],\n",
    "                \"exp_soc_util_performance\": [],\n",
    "                \"est_std_error\": [],\n",
    "                \"variance_soc_util_performance_values\": [],\n",
    "                \"dt\": [],\n",
    "                \"pref_voting_version\": [],\n",
    "            }\n",
    "    \n",
    "    for result_idx, (mean_s, est_std_errors,  variance_soc_util_performance_values, num_trials) in enumerate(results):\n",
    "        print(\"result_idx\", result_idx)\n",
    "        print(mean_s)\n",
    "        for vmidx, vm in enumerate(vms + prob_vms + grade_vms):\n",
    "            key = chunk[result_idx]\n",
    "            print(mean_s)\n",
    "            num_cands, num_voters,num_dims,correlation,rel_dispersion,num_dims_polarized, subpopulation_std, polarization_distance,num_centrist_cands, prob_centrist_voters, voter_utility_name, normalization  = key\n",
    "\n",
    "            data_for_df[\"num_cands\"].append(num_cands)\n",
    "            data_for_df[\"num_voters\"].append(num_voters)\n",
    "            data_for_df[\"num_dims\"].append(num_dims)\n",
    "            data_for_df[\"rel_dispersion\"].append(rel_dispersion)\n",
    "            data_for_df[\"correlation\"].append(correlation)\n",
    "            data_for_df[\"voter_utility\"].append(voter_utility_name)\n",
    "            data_for_df[\"num_dims_polarized\"].append(num_dims_polarized)\n",
    "            data_for_df[\"subpopulation_std\"].append(subpopulation_std)\n",
    "            data_for_df[\"polarization_distance\"].append(polarization_distance)\n",
    "            data_for_df[\"num_centrist_cands\"].append(num_centrist_cands)\n",
    "            data_for_df[\"prob_centrist_voters\"].append(prob_centrist_voters)\n",
    "            data_for_df[\"normalization\"].append(normalization)\n",
    "            data_for_df[\"initial_trials\"].append(initial_trials)\n",
    "            data_for_df[\"step_trials\"].append(step_trials)\n",
    "            data_for_df[\"min_num_trials\"].append(min_num_trials)\n",
    "            data_for_df[\"max_std_error\"].append(max_std_error)\n",
    "            data_for_df[\"num_trials\"].append(num_trials)\n",
    "            data_for_df[\"vm\"].append(vm.name)\n",
    "            data_for_df[\"exp_soc_util_performance\"].append(mean_s[vmidx])\n",
    "            data_for_df[\"est_std_error\"].append(est_std_errors[vmidx])\n",
    "            data_for_df[\"variance_soc_util_performance_values\"].append(variance_soc_util_performance_values[vmidx])\n",
    "            data_for_df[\"dt\"].append(datetime.datetime.now())\n",
    "            data_for_df[\"pref_voting_version\"].append(pref_voting_version)\n",
    "            print(data_for_df)\n",
    "    df = pd.DataFrame(data_for_df)\n",
    "    df.to_csv(f\"{data_dir}/exp_soc_util_performance_simulation_{chunk_num}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd8213c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
