{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08a53e1",
   "metadata": {},
   "source": [
    "# Majority Graph Statistics\n",
    "\n",
    "This notebook contains the code to generate Table 1 in the paper \"The Social Utility of Voting Revisited\" by Wesley H. Holliday and Eric Pacuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c41b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install pref_voting\n",
    "\n",
    "import pref_voting\n",
    "\n",
    "from pref_voting.generate_spatial_profiles import *\n",
    "from pref_voting.utility_profiles import *\n",
    "from pref_voting.utility_functions import *\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from scipy.stats import binomtest\n",
    "from functools import partial\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pref_voting.__version__)\n",
    "\n",
    "num_cpus = os.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5196607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_order = nx.DiGraph()\n",
    "linear_order.add_nodes_from(range(4))\n",
    "linear_order.add_edges_from([(0,1),(0,2), (0,3), (1,2), (1,3), (2,3)]) \n",
    "\n",
    "print(\"Linear order:\")\n",
    "pos = nx.spring_layout(linear_order)\n",
    "nx.draw(linear_order, pos, with_labels=True, node_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_cycle = nx.DiGraph()\n",
    "bottom_cycle.add_nodes_from(range(4))\n",
    "bottom_cycle.add_edges_from([(0,1),(0,2), (0,3), (1,2), (2,3), (3,1)])\n",
    "\n",
    "print(\"Bottom cycle:\")\n",
    "pos = nx.spring_layout(bottom_cycle)\n",
    "nx.draw(bottom_cycle, pos, with_labels=True, node_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0334a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cycle = nx.DiGraph()\n",
    "top_cycle.add_nodes_from(range(4))\n",
    "top_cycle.add_edges_from([(0,1),(1,2), (2,0), (0,3), (1,3), (2,3)])\n",
    "\n",
    "print(\"Top cycle:\")\n",
    "pos = nx.spring_layout(top_cycle)\n",
    "nx.draw(top_cycle, pos, with_labels=True, node_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be57a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_cycle = nx.DiGraph()\n",
    "four_cycle.add_nodes_from(range(4))\n",
    "four_cycle.add_edges_from([(0,1),(1,2), (2,3), (3,0), (2,0), (1,3)])\n",
    "\n",
    "print(\"Four cycle:\")\n",
    "pos = nx.spring_layout(four_cycle)\n",
    "nx.draw(four_cycle, pos, with_labels=True, node_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea7f006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial_confidence_interval(xs, confidence_level=0.95):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        binom_ci = binomtest(int(np.sum(xs)), len(xs)).proportion_ci(\n",
    "            confidence_level=confidence_level, \n",
    "            method='exact'\n",
    "        )\n",
    "    return (binom_ci.low, binom_ci.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b033e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_majority_graph_data(num_voters, num_dims, util_fn, t):\n",
    "        sp = generate_spatial_profile(num_cands=4, num_voters=num_voters, num_dims=num_dims)\n",
    "        up = sp.to_utility_profile(utility_function=util_fn)\n",
    "        prof = up.to_ranking_profile()\n",
    "        mg = prof.margin_graph()\n",
    "\n",
    "        g = nx.DiGraph()\n",
    "        g.add_nodes_from(range(4))\n",
    "        directed_edges = [(a,b) for (a,b,c) in mg.edges]\n",
    "        g.add_edges_from(directed_edges)\n",
    "\n",
    "        return {\n",
    "            \"linear_order\": int(nx.is_isomorphic(g, linear_order)),\n",
    "            \"bottom_cycle\": int(nx.is_isomorphic(g, bottom_cycle)),\n",
    "            \"top_cycle\": int(nx.is_isomorphic(g, top_cycle)),\n",
    "            \"four_cycle\": int(nx.is_isomorphic(g, four_cycle)),\n",
    "            \"other\": int(not (nx.is_isomorphic(g, linear_order) or nx.is_isomorphic(g, bottom_cycle) or nx.is_isomorphic(g, top_cycle) or nx.is_isomorphic(g, four_cycle)))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ca78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_graph_data(\n",
    "    utility_functions=[linear_utility, quadratic_utility, shepsle_utility, \n",
    "                      rm_utility, matthews_utility, mixed_rm_utility],\n",
    "    numbers_of_voters=[11, 101, 1001],\n",
    "    numbers_of_dims=[2, 4, 8],\n",
    "    min_num_samples=10_000,  # Minimum total samples required\n",
    "    increment_size=1_000,    # How many samples to add each iteration\n",
    "    max_num_samples=100_000_000,\n",
    "    max_error=0.001,\n",
    "    use_parallel=True,\n",
    "    num_cpus=23,\n",
    "    verbose=False\n",
    "):\n",
    "    if use_parallel:\n",
    "        pool = Pool(num_cpus)\n",
    "\n",
    "    # Initialize the dataframe dictionary with columns for each outcome type and their errors\n",
    "    dict_for_df = {\n",
    "        \"num_voters\": [], \"num_dims\": [], \"utility function\": [],\n",
    "        \"linear_order\": [], \"bottom_cycle\": [], \"top_cycle\": [],\n",
    "        \"four_cycle\": [], \"other\": [], \n",
    "        \"num_samples\": [], \"error\": [],\n",
    "\n",
    "        # Individual errors for each outcome type\n",
    "        \"error_linear_order\": [],\n",
    "        \"error_bottom_cycle\": [],\n",
    "        \"error_top_cycle\": [],\n",
    "        \"error_four_cycle\": [],\n",
    "        \"error_other\": []\n",
    "    }\n",
    "\n",
    "    outcome_keys = ['linear_order', 'bottom_cycle', 'top_cycle', 'four_cycle', 'other']\n",
    "\n",
    "    for num_voters in numbers_of_voters:\n",
    "        for num_dims in numbers_of_dims:\n",
    "            for util_fn in utility_functions:\n",
    "                print(f\"Voters: {num_voters}, Dimensions: {num_dims}, Utility: {util_fn.__name__}\")\n",
    "                \n",
    "                get_data = partial(\n",
    "                    record_majority_graph_data,\n",
    "                    num_voters,\n",
    "                    num_dims,\n",
    "                    util_fn\n",
    "                )\n",
    "\n",
    "                num_samples = 0\n",
    "                results = {key: [] for key in outcome_keys}\n",
    "                error_ranges = [(0, np.inf)] * 5  # One for each type initially.\n",
    "\n",
    "                # Continue sampling until min_num_samples is reached and \n",
    "                # error requirements are satisfied (or until max_num_samples is reached).\n",
    "                while num_samples < min_num_samples or (any([(err[1] - err[0]) > max_error for err in error_ranges]) and num_samples < max_num_samples):\n",
    "                    if use_parallel:\n",
    "                        data = pool.map(get_data, range(increment_size))\n",
    "                    else:\n",
    "                        data = list(map(get_data, range(increment_size)))\n",
    "\n",
    "                    # Accumulate results\n",
    "                    for d in data:\n",
    "                        for key in results:\n",
    "                            results[key].append(d[key])\n",
    "\n",
    "                    # Calculate error ranges for each type\n",
    "                    error_ranges = [\n",
    "                        binomial_confidence_interval(results[key])\n",
    "                        for key in outcome_keys\n",
    "                    ]\n",
    "\n",
    "                    num_samples += increment_size\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"Samples: {num_samples}\")\n",
    "                        errors_list = [err[1] - err[0] for err in error_ranges]\n",
    "                        print(f\"Errors: {errors_list}\")\n",
    "                        print(f\"Continue? {num_samples < min_num_samples or (any([e > max_error for e in errors_list]) and num_samples < max_num_samples)}\")\n",
    "                        print(\"--------------------\")\n",
    "\n",
    "                # Calculate final percentages and add all data at once\n",
    "                dict_for_df[\"num_voters\"].append(num_voters)\n",
    "                dict_for_df[\"num_dims\"].append(num_dims)\n",
    "                dict_for_df[\"utility function\"].append(util_fn.__name__)\n",
    "                dict_for_df[\"num_samples\"].append(num_samples)\n",
    "                \n",
    "                # Add percentages for each type\n",
    "                for key in outcome_keys:\n",
    "                    percentage = (sum(results[key]) / num_samples)\n",
    "                    dict_for_df[key].append(percentage)\n",
    "                \n",
    "                # Compute observed errors for each category\n",
    "                observed_errors = [(err[1] - err[0]) for err in error_ranges]\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"Final num samples:\", num_samples)\n",
    "                    print(\"Final observed errors:\", observed_errors)\n",
    "                    print(\"-------------------------------\")\n",
    "                    print(\"\")\n",
    "\n",
    "                # Add the observed maximum error (largest among the five)\n",
    "                observed_max_error = max(observed_errors)\n",
    "                dict_for_df[\"error\"].append(observed_max_error)\n",
    "\n",
    "                # Store individual observed errors per category\n",
    "                dict_for_df[\"error_linear_order\"].append(observed_errors[0])\n",
    "                dict_for_df[\"error_bottom_cycle\"].append(observed_errors[1])\n",
    "                dict_for_df[\"error_top_cycle\"].append(observed_errors[2])\n",
    "                dict_for_df[\"error_four_cycle\"].append(observed_errors[3])\n",
    "                dict_for_df[\"error_other\"].append(observed_errors[4])\n",
    "        \n",
    "    return pd.DataFrame(dict_for_df)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "df = majority_graph_data(\n",
    "    min_num_samples=10_000,      # Must have at least 10,000 samples\n",
    "    increment_size=1_000,        # Add 1,000 samples at a time\n",
    "    max_num_samples=100_000_000,\n",
    "    max_error=0.001,\n",
    "    use_parallel=True\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82a86748",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"tournaments_from_spatial_profiles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca3d283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to LaTeX\n",
    "latex_table = df_latex.to_latex(\n",
    "    index=False,\n",
    "    escape=False,\n",
    "    column_format=\"rrlrrrr\"\n",
    ")\n",
    "\n",
    "# Move caption below table and add size reduction and column spacing\n",
    "latex_table = latex_table.replace(\n",
    "    '\\\\begin{table}',\n",
    "    '\\\\begin{table}[t]'  # Add [t] to help with float placement\n",
    ")\n",
    "\n",
    "# Print and save\n",
    "print(latex_table)\n",
    "with open('tournaments_from_spatial_profiles.tex', 'w') as f:\n",
    "    f.write(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimum samples used: {df['num_samples'].min():,}\")\n",
    "print(f\"Maximum samples used: {df['num_samples'].max():,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
